{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability\n",
    "To determine which proteins are more important for the model, we use feature importance techniques. \n",
    "\n",
    "In this case we are using a neural network, direct feature importance isn't as straightforward as with decision trees or linear models. However, we can approximate feature importance using techniques like:\n",
    "\n",
    "1. Permutation Feature Importance: This method involves measuring the change in the model's performance when a single feature value is randomly shuffled.\n",
    "\n",
    "2. SHAP (SHapley Additive exPlanations) Values: This method explains the output of a machine learning model by computing the contribution of each feature to the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Names\n",
    "Extract the correct feature names from the dataset.\n",
    "\n",
    "\n",
    "Load and Process the Data: load the data, extract features and labels, and prepare the training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'C:/Users/a121160/OneDrive - Eviden/Documents/Proyecto/ELMUMY/WPs/WP4/DATA/Proteomic_data_testing.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Extract the feature names\n",
    "metadata_cols = ['Accession_ID', 'Gene_Names', 'Gene_Symbol', 'Protein_Description']\n",
    "feature_cols = data.columns[len(metadata_cols):]\n",
    "\n",
    "# Extract features and labels\n",
    "features = data[feature_cols].values\n",
    "labels = [0] * 16 + [1] * 12 + [2] * 16\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.T, labels, test_size=0.2, random_state=4)\n",
    "\n",
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to torch tensors\n",
    "import torch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a121160\\AppData\\Local\\Temp\\ipykernel_16748\\3374194350.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\a121160\\AppData\\Local\\Temp\\ipykernel_16748\\3374194350.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\a121160\\AppData\\Local\\Temp\\ipykernel_16748\\3374194350.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
      "C:\\Users\\a121160\\AppData\\Local\\Temp\\ipykernel_16748\\3374194350.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7298\n",
      "Epoch [20/100], Loss: 0.4823\n",
      "Epoch [30/100], Loss: 0.3461\n",
      "Epoch [40/100], Loss: 0.2543\n",
      "Epoch [50/100], Loss: 0.1727\n",
      "Epoch [60/100], Loss: 0.1081\n",
      "Epoch [70/100], Loss: 0.0681\n",
      "Epoch [80/100], Loss: 0.0609\n",
      "Epoch [90/100], Loss: 0.0508\n",
      "Epoch [100/100], Loss: 0.0380\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SophisticatedNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SophisticatedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        self.fc4 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 3  # Number of classes: MGUS, sMM, MM\n",
    "\n",
    "# Initialize the model\n",
    "model = SophisticatedNN(input_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8296\n",
      "Epoch [20/100], Loss: 0.5357\n",
      "Epoch [30/100], Loss: 0.3310\n",
      "Epoch [40/100], Loss: 0.2241\n",
      "Epoch [50/100], Loss: 0.1631\n",
      "Epoch [60/100], Loss: 0.1264\n",
      "Epoch [70/100], Loss: 0.0788\n",
      "Epoch [80/100], Loss: 0.0544\n",
      "Epoch [90/100], Loss: 0.0690\n",
      "Epoch [100/100], Loss: 0.0515\n"
     ]
    }
   ],
   "source": [
    "# Ensure the model is in evaluation mode\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 3  # Number of classes: MGUS, sMM, MM\n",
    "\n",
    "# Initialize the model\n",
    "model = SophisticatedNN(input_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with loss recording\n",
    "num_epochs = 100\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SHAP Values and Plot\n",
    "Intall \"shap\" librery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting shap\n",
      "  Downloading shap-0.45.1-cp312-cp312-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from shap) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from shap) (1.4.1.post1)\n",
      "Requirement already satisfied: pandas in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from shap) (4.66.2)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from shap) (24.0)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba (from shap)\n",
      "  Downloading numba-0.60.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->shap)\n",
      "  Downloading llvmlite-0.43.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->shap) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->shap) (3.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\a121160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.45.1-cp312-cp312-win_amd64.whl (455 kB)\n",
      "   ---------------------------------------- 0.0/455.7 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 122.9/455.7 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  450.6/455.7 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 455.7/455.7 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading numba-0.60.0-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB 7.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/2.7 MB 5.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.6/2.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.9/2.7 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.1/2.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.0/2.7 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.43.0-cp312-cp312-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/28.1 MB 8.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.7/28.1 MB 8.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.9/28.1 MB 8.5 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.4/28.1 MB 9.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.7/28.1 MB 8.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 2.0/28.1 MB 8.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.2/28.1 MB 7.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.7/28.1 MB 7.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.0/28.1 MB 8.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.4/28.1 MB 8.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.5/28.1 MB 7.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 4.0/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.4/28.1 MB 7.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.6/28.1 MB 7.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.9/28.1 MB 7.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 5.3/28.1 MB 7.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.8/28.1 MB 8.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 6.1/28.1 MB 7.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.5/28.1 MB 8.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.9/28.1 MB 8.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 7.3/28.1 MB 8.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 7.6/28.1 MB 8.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 8.0/28.1 MB 8.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.5/28.1 MB 8.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.7/28.1 MB 8.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 9.1/28.1 MB 8.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 9.5/28.1 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.9/28.1 MB 8.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 10.4/28.1 MB 8.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.9/28.1 MB 8.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 11.4/28.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.6/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.9/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 12.2/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 12.6/28.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.9/28.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 13.2/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.5/28.1 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.9/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 14.3/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 14.7/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 15.1/28.1 MB 9.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 15.4/28.1 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 15.7/28.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 16.1/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.5/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.9/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 17.3/28.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 17.7/28.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 18.1/28.1 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.5/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.9/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 19.3/28.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 19.8/28.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 20.2/28.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 20.8/28.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 21.0/28.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.4/28.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.6/28.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 22.1/28.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 22.4/28.1 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 22.7/28.1 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.2/28.1 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 23.6/28.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.0/28.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.2/28.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.4/28.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.6/28.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.9/28.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.2/28.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.4/28.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.7/28.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.9/28.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.2/28.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.5/28.1 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.5/28.1 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.8/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.0/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.2/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.6/28.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.7/28.1 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.1/28.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.1/28.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.0.0 llvmlite-0.43.0 numba-0.60.0 shap-0.45.1 slicer-0.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\a121160\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install shap matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute SHAP values and plot:\n",
    "\n",
    "Using SHAP's DeepExplainer, we compute the SHAP values for each class\n",
    "We plot the SHAP summary plots for each class (MGUS, sMM, MM) with the correct feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.71it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The shape of the shap_values matrix does not match the shape of the provided data matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_test_np, nsamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)  \u001b[38;5;66;03m# Adjust nsamples for performance\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Plot summary plot for the first class (MGUS)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSHAP Summary Plot for Class 0 (MGUS)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\shap\\plots\\_beeswarm.py:555\u001b[0m, in \u001b[0;36msummary_legacy\u001b[1;34m(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, auto_size_plot, use_log_scale)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, shape_msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Perhaps the extra column in the shap_values matrix is the \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    553\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant offset? Of so just pass shap_values[:,:-1].\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m num_features \u001b[38;5;241m==\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], shape_msg\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    558\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFEATURE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features)])\n",
      "\u001b[1;31mAssertionError\u001b[0m: The shape of the shap_values matrix does not match the shape of the provided data matrix."
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert data to numpy for SHAP\n",
    "X_train_np = X_train.detach().cpu().numpy()\n",
    "X_test_np = X_test.detach().cpu().numpy()\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a function to wrap the model prediction to be compatible with SHAP\n",
    "def model_predict(data):\n",
    "    tensor_data = torch.tensor(data, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        return model(tensor_data).numpy()\n",
    "\n",
    "# Use SHAP KernelExplainer instead of DeepExplainer for PyTorch models\n",
    "explainer = shap.KernelExplainer(model_predict, X_train_np[:100])  # Using a subset for the background\n",
    "shap_values = explainer.shap_values(X_test_np, nsamples=100)  # Adjust nsamples for performance\n",
    "\n",
    "\n",
    "# Plot summary plot for the first class (MGUS)\n",
    "shap.summary_plot(shap_values[0], X_test_np, feature_names=feature_cols, show=False)\n",
    "plt.title('SHAP Summary Plot for Class 0 (MGUS)')\n",
    "plt.show()\n",
    "\n",
    "# Plot summary plot for the second class (sMM)\n",
    "shap.summary_plot(shap_values[1], X_test_np, feature_names=feature_cols, show=False)\n",
    "plt.title('SHAP Summary Plot for Class 1 (sMM)')\n",
    "plt.show()\n",
    "\n",
    "# Plot summary plot for the third class (MM)\n",
    "shap.summary_plot(shap_values[2], X_test_np, feature_names=feature_cols, show=False)\n",
    "plt.title('SHAP Summary Plot for Class 2 (MM)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permuation Feature Importance\n",
    "\n",
    "This method evaluates the importance of each feature by measuring the increase in the model's prediction error when the feature's values are randomly shuffled. \n",
    "The larger the increase in error, the more important the feature is.\n",
    "\n",
    "Permutation Feature Importance\n",
    "We'll use sklearn's permutation_importance function to measure the importance of each feature.\n",
    "\n",
    "Permutation Feature Importance: This method evaluates the importance of each feature by measuring the increase in the model's prediction error when the feature's values are randomly shuffled.\n",
    " The larger the increase in error, the more important the feature is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got <function model_predict at 0x0000023334A8E160> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m predicted\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m importance \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mimportances_mean\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Plot feature importances\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got <function model_predict at 0x0000023334A8E160> instead."
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Use permutation importance\n",
    "def model_predict(X):\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        outputs = model(X_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.numpy()\n",
    "\n",
    "results = permutation_importance(model_predict, X_test.numpy(), y_test.numpy(), n_repeats=10, random_state=44)\n",
    "importance = results.importances_mean\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(importance)), importance)\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
