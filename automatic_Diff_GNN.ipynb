{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines a class named Value that stores a scalar value and its gradient. It's used for automatic differentiation.\n",
    "\n",
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, data, _children=(), _op=''):\n",
    "    self.data = data\n",
    "    self.grad = 0\n",
    "    # internal variables used for autograd graph construction\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op # the op that produced this node, for graphviz / debugging / etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.data: Stores the scalar value.\n",
    "\n",
    "self.grad: Stores the gradient of the scalar value (initialized to 0).\n",
    "\n",
    "self._backward: A function that will be used to compute the gradient during the backward pass.\n",
    "\n",
    "self._prev: A set of parent nodes (used to keep track of the computational graph).\n",
    "\n",
    "self._op: Stores the operation that created this node, useful for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __add__: Defines addition for Value objects.\n",
    "# Converts \"other\" to a \"Value\" if it is not already.\n",
    "# Creates a new \"Value\" that represents the result of the addition.\n",
    "# Defines a \"_backward\" function to propagate the gradients.\n",
    "# Assigns the \"_backward\" function to the output Value.\n",
    "def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += out.grad\n",
    "        other.grad += out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __mul__: Defines multiplication for Value objects.\n",
    "# Similar to addition, but also includes the chain rule for multiplication in _backward\n",
    "\n",
    "\n",
    "def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other.data * out.grad\n",
    "        other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__pow__: Defines the power operation for Value objects.\n",
    "# Only supports integer and float exponents.\n",
    "#Includes the chain rule for the power operation in _backward.\n",
    "\n",
    "def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += (other * self.data**(other-1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELU (rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu: Implements the ReLU activation function.\n",
    "# Sets the gradient to zero if the input is less than zero, otherwise passes the gradient through.\n",
    "def relu(self):\n",
    "    out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += (out.data > 0) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"backward\": Computes the gradients for all nodes in the computational graph.\n",
    "\n",
    "Uses a topological sort to ensure that gradients are computed in the correct order.\n",
    "\n",
    "Initializes the gradient of the starting node to 1 (since the derivative of itself is 1).\n",
    "\n",
    "Applies the \"_backward\" function for each node in the topologically sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    # topological order all of the children in the graph\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    def build_topo(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "                build_topo(child)\n",
    "            topo.append(v)\n",
    "    build_topo(self)\n",
    "\n",
    "    # go one variable at a time and apply the chain rule to get its gradient\n",
    "    self.grad = 1\n",
    "    for v in reversed(topo):\n",
    "        v._backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These methods define additional arithmetic operations (negation, addition, subtraction, multiplication, division) and their corresponding reverse operations.\n",
    "# __repr__: Provides a string representation of the Value object for easy debugging.\n",
    "\n",
    "def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "def __rsub__(self, other): # other - self\n",
    "    return other + (-self)\n",
    "\n",
    "def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "def __rtruediv__(self, other): # other / self\n",
    "    return other * self**-1\n",
    "\n",
    "def __repr__(self):\n",
    "    return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This class Value is designed for a simple automatic differentiation system, allowing youperform operations on scalar values and automatically compute their gradients. This is useful for implementing machine learning algorithms that require gradient-based optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Graph Neural Network (GNN) using the Value class for automatic differentiation.\n",
    "\n",
    " The processinvolves several steps.\n",
    " \n",
    "  GNNs are designed to work with graph-structured data, where nodes represent entities and edges represent relationships between them. \n",
    "  \n",
    "  The Value class need to be adapted to handle the computations in a GNN. \n",
    "  \n",
    "  Hereâ€™s a high-level overview and an example of how to integrate the Value class into a simple GNN framework:\n",
    "\n",
    "High-Level Steps\n",
    "1. Define the Graph Structure: Represent the graph using nodes and edges. It can be with an adjacency matrix or a adjacency list. It can be also provided.\n",
    "\n",
    "2. Node Features and Initialization: Initialize the node features as Value objects.\n",
    "\n",
    "3. Message Passing: Implement message passing where node features are updated based on the features of neighboring nodes.\n",
    "\n",
    "4. Aggregation: Aggregate messages from neighbors.\n",
    "\n",
    "5. Update: Update the node features using the aggregated messages.\n",
    "\n",
    "5. Loss Calculation and Backpropagation: Compute the loss and use the backward method to compute gradients.\n",
    "\n",
    "This example outlines how to integrate the Value class into a basic GNN framework. The GNN layer performs message passing, aggregation, and updates the node features. The loss function is defined, and backpropagation is used to compute gradients. This implementation can be expanded and refined to include more complex GNN architectures and tasks, such as node classification or link prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Graph Structure\n",
    "A simple graph representation with nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, features):\n",
    "        self.features = Value(features)\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, src, dst):\n",
    "        self.src = src\n",
    "        self.dst = dst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Node Features and Initialization\n",
    "Initialize the nodes with some feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [Node(1.0), Node(2.0), Node(3.0)]\n",
    "edges = [Edge(nodes[0], nodes[1]), Edge(nodes[1], nodes[2])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Message Passing\n",
    "Define a simple message-passing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message(src_node, dst_node):\n",
    "    return src_node.features * 0.5  # Simple example where message is half the source node's features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregation\n",
    "Aggregate messages from all neighbors. For simplicity, we'll sum the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(messages):\n",
    "    result = Value(0.0)\n",
    "    for msg in messages:\n",
    "        result = result + msg\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update\n",
    "Update node features based on aggregated messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(node, aggregated_message):\n",
    "    node.features = node.features + aggregated_message  # Simple update rule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 GNN Layer Implementation\n",
    "Combine the above steps into a GNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_layer(nodes, edges):\n",
    "    # Step 1: Message Passing\n",
    "    messages = {node: [] for node in nodes}\n",
    "    for edge in edges:\n",
    "        msg = message(edge.src, edge.dst)\n",
    "        messages[edge.dst].append(msg)\n",
    "    \n",
    "    # Step 2: Aggregation and Update\n",
    "    for node in nodes:\n",
    "        aggregated_message = aggregate(messages[node])\n",
    "        update(node, aggregated_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Calculation and Backpropagation\n",
    "Define a simple loss and perform backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple loss function (e.g., sum of node features)\n",
    "def loss(nodes):\n",
    "    total_loss = Value(0.0)\n",
    "    for node in nodes:\n",
    "        total_loss = total_loss + node.features\n",
    "    return total_loss\n",
    "\n",
    "# Run the GNN layer\n",
    "gnn_layer(nodes, edges)\n",
    "\n",
    "# Compute the loss\n",
    "total_loss = loss(nodes)\n",
    "\n",
    "# Perform backpropagation\n",
    "total_loss.backward()\n",
    "\n",
    "# Print gradients\n",
    "for node in nodes:\n",
    "    print(node.features.grad)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
